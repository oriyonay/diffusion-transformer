{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lTNN62HgKNVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abca9ad-0f10-48d7-94cf-fc6c2075b6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flowers-recognition.zip to /content\n",
            " 93% 209M/225M [00:01<00:00, 134MB/s]\n",
            "100% 225M/225M [00:01<00:00, 144MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Dataset Retrieval\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"oriyonay2\",\"key\":\"ae6fe32a1ad5e9c76204c9c526f4a3c8\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# download and unzip the data\n",
        "!kaggle datasets download -d alxmamaev/flowers-recognition\n",
        "!unzip -qq flowers-recognition.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utilities\n",
        "\n",
        "'''\n",
        "utils.py: general utilities\n",
        "'''\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# neat function for getting the next batch without epochs\n",
        "def infinite_dataloader(dataloader):\n",
        "    while True:\n",
        "        for data in dataloader:\n",
        "            yield data\n",
        "\n",
        "def plot_images(images):\n",
        "    plt.figure(figsize=(32, 32))\n",
        "    images = torch.cat([i for i in images.cpu()], dim=-1)\n",
        "    images = torch.cat([images], dim=-2).permute(1, 2, 0).cpu()\n",
        "    plt.imshow(images)\n",
        "    plt.show()\n",
        "\n",
        "def save_images(images, path, **kwargs):\n",
        "    grid = make_grid(images, **kwargs)\n",
        "    arr = grid.permute(1, 2, 0).cpu().numpy()\n",
        "    im = Image.fromarray(arr)\n",
        "    im.save(path)\n",
        "\n",
        "def get_data(args):\n",
        "    transforms = T.Compose([\n",
        "        T.Resize(80),\n",
        "        T.RandomResizedCrop(args.image_size, scale=(0.8, 1.0)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(0.5, 0.5)\n",
        "    ])\n",
        "    dataset = datasets.ImageFolder(args.dataset_path, transform=transforms)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    dataloader = infinite_dataloader(dataloader)\n",
        "    return dataloader\n",
        "\n",
        "def setup_logging(run_name):\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    os.makedirs(os.path.join('models', run_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join('results', run_name), exist_ok=True)\n",
        "\n",
        "class EMA:\n",
        "    '''\n",
        "    exponential moving average class, for adjusting model weights more smoothly\n",
        "    EMA update: w = (beta * w_old) + ((1 - beta) * w_new)\n",
        "\n",
        "    beta: EMA parameter\n",
        "    step_start_ema: number of warmup steps before using EMA\n",
        "    '''\n",
        "    def __init__(self, beta, step_start_ema=2000):\n",
        "        self.beta = beta\n",
        "        self.step_start_ema = step_start_ema\n",
        "        self.step = 0\n",
        "\n",
        "    def step_ema(self, ema_model, model):\n",
        "        if self.step < self.step_start_ema:\n",
        "            self.reset_params(ema_model, model)\n",
        "        else:\n",
        "            self.update_model_average(ema_model, model)\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "    def update_model_average(self, ema_model, model):\n",
        "        mp = model.parameters()\n",
        "        ep = ema_model.parameters()\n",
        "        for current_param, ema_param in zip(mp, ep):\n",
        "            w_old, w_new = ema_param.data, current_param.data\n",
        "            ema_param.data = self.update_average(w_old, w_new)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        return (self.beta * old) + ((1 - self.beta) * new)\n",
        "\n",
        "    def reset_params(self, ema_model, model):\n",
        "        ema_model.load_state_dict(model.state_dict())"
      ],
      "metadata": {
        "id": "2BdJTqw0KYsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Models\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class Diffusion:\n",
        "    '''\n",
        "    the diffusion class\n",
        "\n",
        "    noise_steps: number of diffusion noising steps\n",
        "    betas: start and end for the variance schedule\n",
        "    img_size: generated image size\n",
        "    device: the device to use for training\n",
        "    '''\n",
        "    def __init__(self, noise_steps=1000, betas=(1e-4, 2e-2), img_size=64, device='cuda'):\n",
        "        self.noise_steps = noise_steps\n",
        "        self.betas = betas\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "        # prepare the noise variance schedule and various constants:\n",
        "        self.beta = self.prepare_noise_schedule().to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat)\n",
        "        self.sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat)\n",
        "\n",
        "    # computes a linear noise schedule\n",
        "    def prepare_noise_schedule(self):\n",
        "        return torch.linspace(*self.betas, self.noise_steps)\n",
        "\n",
        "    # noises the data x to timestep t\n",
        "    def noise_data(self, x, t):\n",
        "        # get constants\n",
        "        sqrt_alpha_hat = self.sqrt_alpha_hat[t][:, None, None, None]\n",
        "        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat[t][:, None, None, None]\n",
        "\n",
        "        # generate noise\n",
        "        e = torch.randn_like(x)\n",
        "\n",
        "        # compute scaled signal and noise\n",
        "        signal = sqrt_alpha_hat * x\n",
        "        noise = sqrt_one_minus_alpha_hat * e\n",
        "        return signal + noise, e\n",
        "\n",
        "    # samples time steps (just returns random ints as timesteps)\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(1, self.noise_steps, size=(n,))\n",
        "\n",
        "    # sample n datapoints from the model\n",
        "    # labels: optional labels if the model was trained conditionally\n",
        "    # cfg_scale:\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, n, labels=None, cfg_scale=3):\n",
        "        model.eval()\n",
        "\n",
        "        # start with gaussian noise\n",
        "        x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
        "\n",
        "        # denoise data\n",
        "        for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "            # create the timestep tensor that encodes the current timestep\n",
        "            t = (torch.ones(n) * i).long().to(self.device)\n",
        "\n",
        "            # predict the noise\n",
        "            predicted_noise = model(x, t, labels)\n",
        "\n",
        "            # classifier-free guidance: linearly interpolate between\n",
        "            # unconditional and conditional (above) samples\n",
        "            if labels:\n",
        "                uncond_predicted_noise = model(x, t)\n",
        "                predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
        "\n",
        "            # compute scaling constants\n",
        "            alpha = self.alpha[t][:, None, None, None]\n",
        "            alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "            beta = self.beta[t][:, None, None, None]\n",
        "\n",
        "            # remove a small bit of noise from x\n",
        "            noise = torch.randn_like(x) if i > 1 else torch.zeros_like(x)\n",
        "            signal_scale = 1 / torch.sqrt(alpha)\n",
        "            pred_noise_scale = (1 - alpha) / (torch.sqrt(1 - alpha_hat))\n",
        "            scaled_noise = torch.sqrt(beta) * noise\n",
        "            signal = x - (pred_noise_scale * predicted_noise)\n",
        "\n",
        "            x = (signal_scale * signal) + scaled_noise\n",
        "\n",
        "        # set the model back to training mode\n",
        "        model.train()\n",
        "\n",
        "        # clamp and rescale x values to [0, 1] (output was [-1, 1]):\n",
        "        x = (x.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "        # convert x to valid pixel range:\n",
        "        x = (x * 255).type(torch.uint8)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchingLayer(nn.Module):\n",
        "    '''\n",
        "    Splits the input image into non-overlapping patches.\n",
        "    '''\n",
        "    def __init__(self, patch_size, num_channels):\n",
        "        super(PatchingLayer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Split the image into patches\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        x = x.view(b, -1, c * self.patch_size * self.patch_size)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, E = x.size()  # Batch size, Number of tokens, Embedding dimension\n",
        "        pos = torch.arange(0, N).unsqueeze(0).unsqueeze(-1).to(x.device).float()  # Shape: [1, N, 1]\n",
        "        div_term = torch.exp(torch.arange(0, E, 2).float() * -(math.log(10000.0) / E)).to(x.device)  # Shape: [E//2]\n",
        "        div_term = div_term.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, E//2]\n",
        "\n",
        "        pos_enc = torch.zeros_like(x)  # Shape: [B, N, E]\n",
        "\n",
        "        pos_enc[:, :, 0::2] = torch.sin(pos * div_term)  # Apply to even indices\n",
        "        pos_enc[:, :, 1::2] = torch.cos(pos * div_term)  # Apply to odd indices\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self, d_model, patch_size, num_channels):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model * 4, num_channels * patch_size * patch_size)\n",
        "        )\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape tokens back to patches\n",
        "        x = self.mlp(x)\n",
        "        b, n, _ = x.size()\n",
        "        x = x.view(b, n, self.num_channels, self.patch_size, self.patch_size)\n",
        "\n",
        "        # Reconstruct the original image dimensions from the patches\n",
        "        h_dim = w_dim = int((n)**0.5)\n",
        "        x = x.view(b, h_dim, w_dim, self.num_channels, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
        "        x = x.view(b, self.num_channels, h_dim * self.patch_size, w_dim * self.patch_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImageTransformer(nn.Module):\n",
        "    '''\n",
        "    Full model architecture\n",
        "    '''\n",
        "    def __init__(self, d_model, nhead, num_layers, patch_size, num_classes=None, num_channels=3, dropout=0.05):\n",
        "        super(ImageTransformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.patching_layer = PatchingLayer(patch_size, num_channels)\n",
        "        self.projection = nn.Linear(num_channels * patch_size * patch_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, nhead, batch_first=True, dropout=dropout),\n",
        "            num_layers\n",
        "        )\n",
        "        self.output_layer = OutputLayer(d_model, patch_size, num_channels)\n",
        "\n",
        "        if num_classes:\n",
        "            self.label_emb = nn.Embedding(num_classes, d_model)\n",
        "\n",
        "    def forward(self, x, t, label=None):\n",
        "        # compute positional encoding for the timestep (len(t), self.time_dim)\n",
        "        t = t.unsqueeze(-1).float()\n",
        "        t = self._time_embedding(t)\n",
        "\n",
        "        # class-conditioning\n",
        "        if label:\n",
        "            t = t + self.label_emb(label)\n",
        "\n",
        "        x = self.patching_layer(x)\n",
        "        x = self.projection(x)\n",
        "        residual = x\n",
        "        x = x + t + self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x) + residual\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def _time_embedding(self, t):\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.d_model, 2, dtype=torch.float32).to(t.device) / self.d_model))\n",
        "\n",
        "        # Create the sine and cosine encodings\n",
        "        pos_enc_sin = torch.sin(t.unsqueeze(1).float() * inv_freq)\n",
        "        pos_enc_cos = torch.cos(t.unsqueeze(1).float() * inv_freq)\n",
        "\n",
        "        # Concatenate the sine and cosine encodings\n",
        "        pos_enc = torch.cat([pos_enc_sin, pos_enc_cos], dim=-1)\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "    @property\n",
        "    def n_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "sQwDuF2pKhme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WapTp0-EmQgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training\n",
        "\n",
        "'''\n",
        "training the diffusion model\n",
        "'''\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    print(f'Training on {args.device}')\n",
        "\n",
        "    # set up training run\n",
        "    setup_logging(args.run_name)\n",
        "    device = args.device\n",
        "    dataloader = get_data(args)\n",
        "    model = ImageTransformer(\n",
        "        d_model=128,\n",
        "        nhead=8,\n",
        "        num_layers=4,\n",
        "        patch_size=4,\n",
        "        num_channels=3\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
        "    ema = EMA(args.ema_beta, args.step_start_ema)\n",
        "    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
        "\n",
        "    print(f'Model contains {model.n_params} trainable parameters')\n",
        "\n",
        "    pbar = trange(args.n_iters)\n",
        "    for i in pbar:\n",
        "        # prepare next batch\n",
        "        images, labels = next(dataloader)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device) if args.n_labels else None\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # set labels to None 10% of the time, even if this is\n",
        "        # conditional generation (classifier-free guidance)\n",
        "        if np.random.random() < 0.1:\n",
        "            labels = None\n",
        "\n",
        "        # forward pass\n",
        "        t = diffusion.sample_timesteps(batch_size).to(device)\n",
        "        x_t, noise = diffusion.noise_data(images, t)\n",
        "        predicted_noise = model(x_t, t, labels)\n",
        "        loss = criterion(noise, predicted_noise)\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ema.step_ema(ema_model, model)\n",
        "\n",
        "        # update progress bar\n",
        "        pbar.set_description(f'loss: {loss.item():.3f}')\n",
        "\n",
        "        # sample images every few iterations\n",
        "        if (i+1) % args.checkpoint_every == 0:\n",
        "            sampled_images = diffusion.sample(model, n=batch_size)\n",
        "            save_path = os.path.join('results', args.run_name, f'{i:06}.jpg')\n",
        "            save_images(sampled_images, save_path)\n",
        "\n",
        "            # checkpoint the model weights\n",
        "            torch.save(model.state_dict(), os.path.join('models', args.run_name, 'checkpoint.pt'))\n",
        "\n",
        "class Arguments:\n",
        "    pass\n",
        "\n",
        "def launch():\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    args = Arguments() # parser.parse_args()\n",
        "\n",
        "    args.run_name = 'image_transformer_test'\n",
        "    args.n_iters = 40000\n",
        "    args.batch_size = 16\n",
        "    args.image_size = 64\n",
        "    args.checkpoint_every = 1000\n",
        "    args.n_labels = None # number of labels for conditional generation\n",
        "    args.dataset_path = 'flowers'\n",
        "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    args.lr = 3e-4\n",
        "    args.ema_beta = 0.995\n",
        "    args.step_start_ema = 2000 # warmup steps before EMA\n",
        "\n",
        "    train(args)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    launch()"
      ],
      "metadata": {
        "id": "YlkqRm-NK_0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "c390f5f7-8144-4700-b7c5-e2fc115c8033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda\n",
            "Model contains 2469040 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "999it [00:10, 96.78it/s]\n",
            "999it [00:10, 95.40it/s]\n",
            "999it [00:10, 95.28it/s]\n",
            "loss: 0.055:   9%|▉         | 3664/40000 [05:39<56:10, 10.78it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f79e433c3ae9>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f79e433c3ae9>\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_start_ema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;31m# warmup steps before EMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f79e433c3ae9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_ema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mema_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}