{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6cf759d5b4cc423a96b1d944fe05883b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60f47695ddaa4ce5ab17d297541c0107",
              "IPY_MODEL_3d141a0355d84966bdd350f3f4b29aa8",
              "IPY_MODEL_bb9b4cade5374fa79bcdcbe319ada058"
            ],
            "layout": "IPY_MODEL_39dcdcd64ba4422eb3faff113d9c1b5f"
          }
        },
        "60f47695ddaa4ce5ab17d297541c0107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4139f05931542919403440e427216a1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_34b627b705bb4e8b9ba57e25d4b8e8c3",
            "value": ""
          }
        },
        "3d141a0355d84966bdd350f3f4b29aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ca580571bb4eb2aa1779179946aac6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3eb45f21b62f48c89b7652d273099924",
            "value": 1
          }
        },
        "bb9b4cade5374fa79bcdcbe319ada058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3879bea10a234cf6a70d496fcb6549f6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a78709ee4e4343d7aed04b00a1bc0258",
            "value": " 999/? [00:35&lt;00:00, 27.64it/s]"
          }
        },
        "39dcdcd64ba4422eb3faff113d9c1b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4139f05931542919403440e427216a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34b627b705bb4e8b9ba57e25d4b8e8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82ca580571bb4eb2aa1779179946aac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3eb45f21b62f48c89b7652d273099924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3879bea10a234cf6a70d496fcb6549f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78709ee4e4343d7aed04b00a1bc0258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "\n",
        "model_path = 'checkpoint.pt' # path to model checkpoint\n",
        "n_samples = 2048 # number of samples to generate\n",
        "generate_batch_size = 1024 # number of fakes to sample simultaneously\n",
        "data_loader_batch_size = 128 # batch size for real images\n",
        "fid_feature = 2048 # an integer will indicate the inceptionv3 feature layer to choose (64, 192, 768, or 2048)"
      ],
      "metadata": {
        "id": "3f2ALjt-y6Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing dependencies\n",
        "!pip3 install -q torchmetrics torch-fidelity"
      ],
      "metadata": {
        "id": "dyGDcAyhPc-s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RFqHbmHJO080"
      },
      "outputs": [],
      "source": [
        "#@title Models\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class Diffusion:\n",
        "    '''\n",
        "    the diffusion class\n",
        "\n",
        "    noise_steps: number of diffusion noising steps\n",
        "    betas: start and end for the variance schedule\n",
        "    img_size: generated image size\n",
        "    device: the device to use for training\n",
        "    '''\n",
        "    def __init__(self, noise_steps=1000, betas=(1e-4, 2e-2), img_size=64, device='cuda'):\n",
        "        self.noise_steps = noise_steps\n",
        "        self.betas = betas\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "        # prepare the noise variance schedule and various constants:\n",
        "        self.beta = self.prepare_noise_schedule().to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat)\n",
        "        self.sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat)\n",
        "\n",
        "    # computes a linear noise schedule\n",
        "    def prepare_noise_schedule(self):\n",
        "        return torch.linspace(*self.betas, self.noise_steps)\n",
        "\n",
        "    # noises the data x to timestep t\n",
        "    def noise_data(self, x, t):\n",
        "        # get constants\n",
        "        sqrt_alpha_hat = self.sqrt_alpha_hat[t][:, None, None, None]\n",
        "        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat[t][:, None, None, None]\n",
        "\n",
        "        # generate noise\n",
        "        e = torch.randn_like(x)\n",
        "\n",
        "        # compute scaled signal and noise\n",
        "        signal = sqrt_alpha_hat * x\n",
        "        noise = sqrt_one_minus_alpha_hat * e\n",
        "        return signal + noise, e\n",
        "\n",
        "    # samples time steps (just returns random ints as timesteps)\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(1, self.noise_steps, size=(n,))\n",
        "\n",
        "    # sample n datapoints from the model\n",
        "    # labels: optional labels if the model was trained conditionally\n",
        "    # cfg_scale:\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, n, labels=None, cfg_scale=3):\n",
        "        model.eval()\n",
        "\n",
        "        # start with gaussian noise\n",
        "        x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
        "\n",
        "        # denoise data\n",
        "        for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "            # create the timestep tensor that encodes the current timestep\n",
        "            t = (torch.ones(n) * i).long().to(self.device)\n",
        "\n",
        "            # predict the noise\n",
        "            predicted_noise = model(x, t, labels)\n",
        "\n",
        "            # classifier-free guidance: linearly interpolate between\n",
        "            # unconditional and conditional (above) samples\n",
        "            if labels:\n",
        "                uncond_predicted_noise = model(x, t)\n",
        "                predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
        "\n",
        "            # compute scaling constants\n",
        "            alpha = self.alpha[t][:, None, None, None]\n",
        "            alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "            beta = self.beta[t][:, None, None, None]\n",
        "\n",
        "            # remove a small bit of noise from x\n",
        "            noise = torch.randn_like(x) if i > 1 else torch.zeros_like(x)\n",
        "            signal_scale = 1 / torch.sqrt(alpha)\n",
        "            pred_noise_scale = (1 - alpha) / (torch.sqrt(1 - alpha_hat))\n",
        "            scaled_noise = torch.sqrt(beta) * noise\n",
        "            signal = x - (pred_noise_scale * predicted_noise)\n",
        "\n",
        "            x = (signal_scale * signal) + scaled_noise\n",
        "\n",
        "        # set the model back to training mode\n",
        "        model.train()\n",
        "\n",
        "        # clamp and rescale x values to [0, 1] (output was [-1, 1]):\n",
        "        x = (x.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "        # convert x to valid pixel range:\n",
        "        x = (x * 255).type(torch.uint8)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchingLayer(nn.Module):\n",
        "    '''\n",
        "    Splits the input image into non-overlapping patches.\n",
        "    '''\n",
        "    def __init__(self, patch_size, num_channels):\n",
        "        super(PatchingLayer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Split the image into patches\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        x = x.view(b, -1, c * self.patch_size * self.patch_size)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, E = x.size()  # Batch size, Number of tokens, Embedding dimension\n",
        "        pos = torch.arange(0, N).unsqueeze(0).unsqueeze(-1).to(x.device).float()  # Shape: [1, N, 1]\n",
        "        div_term = torch.exp(torch.arange(0, E, 2).float() * -(math.log(10000.0) / E)).to(x.device)  # Shape: [E//2]\n",
        "        div_term = div_term.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, E//2]\n",
        "\n",
        "        pos_enc = torch.zeros_like(x)  # Shape: [B, N, E]\n",
        "\n",
        "        pos_enc[:, :, 0::2] = torch.sin(pos * div_term)  # Apply to even indices\n",
        "        pos_enc[:, :, 1::2] = torch.cos(pos * div_term)  # Apply to odd indices\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self, d_model, patch_size, num_channels):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model * 4, num_channels * patch_size * patch_size)\n",
        "        )\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape tokens back to patches\n",
        "        x = self.mlp(x)\n",
        "        b, n, _ = x.size()\n",
        "        x = x.view(b, n, self.num_channels, self.patch_size, self.patch_size)\n",
        "\n",
        "        # Reconstruct the original image dimensions from the patches\n",
        "        h_dim = w_dim = int((n)**0.5)\n",
        "        x = x.view(b, h_dim, w_dim, self.num_channels, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
        "        x = x.view(b, self.num_channels, h_dim * self.patch_size, w_dim * self.patch_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImageTransformer(nn.Module):\n",
        "    '''\n",
        "    Full model architecture\n",
        "    '''\n",
        "    def __init__(self, d_model, nhead, num_layers, patch_size, num_classes=None, num_channels=3, dropout=0.05, window_size=4):\n",
        "        super(ImageTransformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.patching_layer = PatchingLayer(patch_size, num_channels)\n",
        "        self.projection = nn.Linear(num_channels * patch_size * patch_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, nhead, batch_first=True, dropout=dropout),\n",
        "            num_layers\n",
        "        )\n",
        "        self.output_layer = OutputLayer(d_model, patch_size, num_channels)\n",
        "\n",
        "        if num_classes:\n",
        "            self.label_emb = nn.Embedding(num_classes, d_model)\n",
        "\n",
        "    def forward(self, x, t, label=None):\n",
        "        # compute positional encoding for the timestep (len(t), self.time_dim)\n",
        "        t = t.unsqueeze(-1).float()\n",
        "        t = self._time_embedding(t)\n",
        "\n",
        "        # class-conditioning\n",
        "        if label:\n",
        "            t = t + self.label_emb(label)\n",
        "\n",
        "        x = self.patching_layer(x)\n",
        "        x = self.projection(x)\n",
        "        residual = x\n",
        "        x = x + t + self.positional_encoding(x)\n",
        "        x = self.encoder(x) + residual\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def _time_embedding(self, t):\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.d_model, 2, dtype=torch.float32).to(t.device) / self.d_model))\n",
        "\n",
        "        # Create the sine and cosine encodings\n",
        "        pos_enc_sin = torch.sin(t.unsqueeze(1).float() * inv_freq)\n",
        "        pos_enc_cos = torch.cos(t.unsqueeze(1).float() * inv_freq)\n",
        "\n",
        "        # Concatenate the sine and cosine encodings\n",
        "        pos_enc = torch.cat([pos_enc_sin, pos_enc_cos], dim=-1)\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "    @property\n",
        "    def n_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "class TBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, patch_size, num_classes=None, num_channels=3, dropout=0.05):\n",
        "        super(TBlock, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.patching_layer = PatchingLayer(patch_size, num_channels)\n",
        "        self.projection = nn.Linear(num_channels * patch_size * patch_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, nhead, batch_first=True, dropout=dropout),\n",
        "            num_layers\n",
        "        )\n",
        "        self.output_layer = OutputLayer(d_model, patch_size, num_channels)\n",
        "\n",
        "        if num_classes:\n",
        "            self.label_emb = nn.Embedding(num_classes, d_model)\n",
        "\n",
        "    def forward(self, x, t, label=None):\n",
        "        # compute positional encoding for the timestep (len(t), self.time_dim)\n",
        "        t = t.unsqueeze(-1).float()\n",
        "        t = self._time_embedding(t)\n",
        "\n",
        "        # class-conditioning\n",
        "        if label:\n",
        "            t = t + self.label_emb(label)\n",
        "\n",
        "        x = self.patching_layer(x)\n",
        "        x = self.projection(x)\n",
        "        residual = x\n",
        "        x = x + t + self.positional_encoding(x)\n",
        "        x = self.encoder(x) + residual\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def _time_embedding(self, t):\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.d_model, 2, dtype=torch.float32).to(t.device) / self.d_model))\n",
        "\n",
        "        # Create the sine and cosine encodings\n",
        "        pos_enc_sin = torch.sin(t.unsqueeze(1).float() * inv_freq)\n",
        "        pos_enc_cos = torch.cos(t.unsqueeze(1).float() * inv_freq)\n",
        "\n",
        "        # Concatenate the sine and cosine encodings\n",
        "        pos_enc = torch.cat([pos_enc_sin, pos_enc_cos], dim=-1)\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "\n",
        "class TNet(nn.Module):\n",
        "    def __init__(self, num_blocks, d_model, nhead, num_layers, patch_size, num_channels=3, dropout=0.05):\n",
        "        super(TNet, self).__init__()\n",
        "\n",
        "        # Encoder and Decoder blocks\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for _ in range(num_blocks * 2):\n",
        "            self.blocks.append(TBlock(d_model, nhead, num_layers, patch_size, num_channels=num_channels, dropout=dropout))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2, 2)  # For downsampling\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')  # For upsampling\n",
        "\n",
        "    def forward(self, x, t, label=None):\n",
        "        # Encoding path with downsampling\n",
        "        encoder_outs = []\n",
        "        for i in range(len(self.blocks) // 2):\n",
        "            x = self.blocks[i](x, t, label)\n",
        "            encoder_outs.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        # Decoding path with upsampling\n",
        "        for i in range(len(self.blocks) // 2, len(self.blocks)):\n",
        "            x = self.upsample(x)\n",
        "            x += encoder_outs.pop()  # Skip-connection\n",
        "            x = self.blocks[i](x, t, label)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def n_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate n samples\n",
        "\n",
        "def generate_samples(diffusion, model, n: int, batch_size: int = 64) -> torch.Tensor:\n",
        "    samples = []\n",
        "\n",
        "    while n > 0:\n",
        "        # Generate the minimum of n or batch_size samples\n",
        "        current_batch_size = min(n, batch_size)\n",
        "        with torch.no_grad():\n",
        "            # Sample images from the model\n",
        "            sample = diffusion.sample(model, n=current_batch_size).cpu()\n",
        "        samples.append(sample)\n",
        "        n -= current_batch_size\n",
        "\n",
        "    all_samples = torch.cat(samples, dim=0)\n",
        "    return all_samples\n",
        "\n",
        "diffusion = Diffusion(img_size=32)\n",
        "\n",
        "if False:\n",
        "    model = TNet(\n",
        "        num_blocks=3,\n",
        "        d_model=128,\n",
        "        nhead=8,\n",
        "        num_layers=3,\n",
        "        patch_size=4,\n",
        "        num_channels=3\n",
        "    ).cuda()\n",
        "else:\n",
        "    model = ImageTransformer(\n",
        "        d_model=128,\n",
        "        nhead=32,\n",
        "        num_layers=4,\n",
        "        patch_size=4,\n",
        "        num_channels=3\n",
        "    ).cuda()\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location='cuda'))\n",
        "model.eval()\n",
        "\n",
        "# Generate samples\n",
        "generated_samples = generate_samples(diffusion, model, n=n_samples, batch_size=generate_batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Eo1xdLnst3jN",
        "outputId": "b03c8f23-9ba1-412f-9109-2914d8fbbaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "999it [03:29,  4.77it/s]\n",
            "999it [03:32,  4.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('a.p', 'rb') as f:\n",
        "    generated_samples = pickle.load(f)"
      ],
      "metadata": {
        "id": "SlecZ6tolcww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute FID\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance as FID\n",
        "\n",
        "def float2int(images):\n",
        "    images = (images.clamp(-1, 1) + 1) / 2\n",
        "    images = (images * 255).type(torch.uint8)\n",
        "    return images\n",
        "\n",
        "transforms = T.Compose([\n",
        "    T.Resize(32),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "val_dataset = datasets.CIFAR10('.', train=True, transform=transforms, download=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=data_loader_batch_size, shuffle=True)\n",
        "\n",
        "fid = FID(feature=fid_feature).cuda()\n",
        "\n",
        "left = len(val_dataset) # n_samples\n",
        "while left > 0:\n",
        "    n_generate = min(left, data_loader_batch_size)\n",
        "    reals = float2int(next(b for b in val_dataloader)[0]).cuda()\n",
        "    fid.update(reals, real=True)\n",
        "    left -= n_generate\n",
        "    print(f'{left} real images left')\n",
        "    del reals\n",
        "\n",
        "for i in range(0, n_samples, data_loader_batch_size):\n",
        "    fakes = generated_samples[i:i+data_loader_batch_size].cuda()\n",
        "    fid.update(fakes, real=False)\n",
        "    del fakes\n",
        "# fid.update(generated_samples.cuda(), real=False)\n",
        "score = fid.compute().item()\n",
        "fid.reset()\n",
        "print(f'FID: {score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR8EcaGnvY6m",
        "outputId": "7d575d5c-dfeb-4a4f-b899-bf2a6fee9fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "49872 real images left\n",
            "49744 real images left\n",
            "49616 real images left\n",
            "49488 real images left\n",
            "49360 real images left\n",
            "49232 real images left\n",
            "49104 real images left\n",
            "48976 real images left\n",
            "48848 real images left\n",
            "48720 real images left\n",
            "48592 real images left\n",
            "48464 real images left\n",
            "48336 real images left\n",
            "48208 real images left\n",
            "48080 real images left\n",
            "47952 real images left\n",
            "47824 real images left\n",
            "47696 real images left\n",
            "47568 real images left\n",
            "47440 real images left\n",
            "47312 real images left\n",
            "47184 real images left\n",
            "47056 real images left\n",
            "46928 real images left\n",
            "46800 real images left\n",
            "46672 real images left\n",
            "46544 real images left\n",
            "46416 real images left\n",
            "46288 real images left\n",
            "46160 real images left\n",
            "46032 real images left\n",
            "45904 real images left\n",
            "45776 real images left\n",
            "45648 real images left\n",
            "45520 real images left\n",
            "45392 real images left\n",
            "45264 real images left\n",
            "45136 real images left\n",
            "45008 real images left\n",
            "44880 real images left\n",
            "44752 real images left\n",
            "44624 real images left\n",
            "44496 real images left\n",
            "44368 real images left\n",
            "44240 real images left\n",
            "44112 real images left\n",
            "43984 real images left\n",
            "43856 real images left\n",
            "43728 real images left\n",
            "43600 real images left\n",
            "43472 real images left\n",
            "43344 real images left\n",
            "43216 real images left\n",
            "43088 real images left\n",
            "42960 real images left\n",
            "42832 real images left\n",
            "42704 real images left\n",
            "42576 real images left\n",
            "42448 real images left\n",
            "42320 real images left\n",
            "42192 real images left\n",
            "42064 real images left\n",
            "41936 real images left\n",
            "41808 real images left\n",
            "41680 real images left\n",
            "41552 real images left\n",
            "41424 real images left\n",
            "41296 real images left\n",
            "41168 real images left\n",
            "41040 real images left\n",
            "40912 real images left\n",
            "40784 real images left\n",
            "40656 real images left\n",
            "40528 real images left\n",
            "40400 real images left\n",
            "40272 real images left\n",
            "40144 real images left\n",
            "40016 real images left\n",
            "39888 real images left\n",
            "39760 real images left\n",
            "39632 real images left\n",
            "39504 real images left\n",
            "39376 real images left\n",
            "39248 real images left\n",
            "39120 real images left\n",
            "38992 real images left\n",
            "38864 real images left\n",
            "38736 real images left\n",
            "38608 real images left\n",
            "38480 real images left\n",
            "38352 real images left\n",
            "38224 real images left\n",
            "38096 real images left\n",
            "37968 real images left\n",
            "37840 real images left\n",
            "37712 real images left\n",
            "37584 real images left\n",
            "37456 real images left\n",
            "37328 real images left\n",
            "37200 real images left\n",
            "37072 real images left\n",
            "36944 real images left\n",
            "36816 real images left\n",
            "36688 real images left\n",
            "36560 real images left\n",
            "36432 real images left\n",
            "36304 real images left\n",
            "36176 real images left\n",
            "36048 real images left\n",
            "35920 real images left\n",
            "35792 real images left\n",
            "35664 real images left\n",
            "35536 real images left\n",
            "35408 real images left\n",
            "35280 real images left\n",
            "35152 real images left\n",
            "35024 real images left\n",
            "34896 real images left\n",
            "34768 real images left\n",
            "34640 real images left\n",
            "34512 real images left\n",
            "34384 real images left\n",
            "34256 real images left\n",
            "34128 real images left\n",
            "34000 real images left\n",
            "33872 real images left\n",
            "33744 real images left\n",
            "33616 real images left\n",
            "33488 real images left\n",
            "33360 real images left\n",
            "33232 real images left\n",
            "33104 real images left\n",
            "32976 real images left\n",
            "32848 real images left\n",
            "32720 real images left\n",
            "32592 real images left\n",
            "32464 real images left\n",
            "32336 real images left\n",
            "32208 real images left\n",
            "32080 real images left\n",
            "31952 real images left\n",
            "31824 real images left\n",
            "31696 real images left\n",
            "31568 real images left\n",
            "31440 real images left\n",
            "31312 real images left\n",
            "31184 real images left\n",
            "31056 real images left\n",
            "30928 real images left\n",
            "30800 real images left\n",
            "30672 real images left\n",
            "30544 real images left\n",
            "30416 real images left\n",
            "30288 real images left\n",
            "30160 real images left\n",
            "30032 real images left\n",
            "29904 real images left\n",
            "29776 real images left\n",
            "29648 real images left\n",
            "29520 real images left\n",
            "29392 real images left\n",
            "29264 real images left\n",
            "29136 real images left\n",
            "29008 real images left\n",
            "28880 real images left\n",
            "28752 real images left\n",
            "28624 real images left\n",
            "28496 real images left\n",
            "28368 real images left\n",
            "28240 real images left\n",
            "28112 real images left\n",
            "27984 real images left\n",
            "27856 real images left\n",
            "27728 real images left\n",
            "27600 real images left\n",
            "27472 real images left\n",
            "27344 real images left\n",
            "27216 real images left\n",
            "27088 real images left\n",
            "26960 real images left\n",
            "26832 real images left\n",
            "26704 real images left\n",
            "26576 real images left\n",
            "26448 real images left\n",
            "26320 real images left\n",
            "26192 real images left\n",
            "26064 real images left\n",
            "25936 real images left\n",
            "25808 real images left\n",
            "25680 real images left\n",
            "25552 real images left\n",
            "25424 real images left\n",
            "25296 real images left\n",
            "25168 real images left\n",
            "25040 real images left\n",
            "24912 real images left\n",
            "24784 real images left\n",
            "24656 real images left\n",
            "24528 real images left\n",
            "24400 real images left\n",
            "24272 real images left\n",
            "24144 real images left\n",
            "24016 real images left\n",
            "23888 real images left\n",
            "23760 real images left\n",
            "23632 real images left\n",
            "23504 real images left\n",
            "23376 real images left\n",
            "23248 real images left\n",
            "23120 real images left\n",
            "22992 real images left\n",
            "22864 real images left\n",
            "22736 real images left\n",
            "22608 real images left\n",
            "22480 real images left\n",
            "22352 real images left\n",
            "22224 real images left\n",
            "22096 real images left\n",
            "21968 real images left\n",
            "21840 real images left\n",
            "21712 real images left\n",
            "21584 real images left\n",
            "21456 real images left\n",
            "21328 real images left\n",
            "21200 real images left\n",
            "21072 real images left\n",
            "20944 real images left\n",
            "20816 real images left\n",
            "20688 real images left\n",
            "20560 real images left\n",
            "20432 real images left\n",
            "20304 real images left\n",
            "20176 real images left\n",
            "20048 real images left\n",
            "19920 real images left\n",
            "19792 real images left\n",
            "19664 real images left\n",
            "19536 real images left\n",
            "19408 real images left\n",
            "19280 real images left\n",
            "19152 real images left\n",
            "19024 real images left\n",
            "18896 real images left\n",
            "18768 real images left\n",
            "18640 real images left\n",
            "18512 real images left\n",
            "18384 real images left\n",
            "18256 real images left\n",
            "18128 real images left\n",
            "18000 real images left\n",
            "17872 real images left\n",
            "17744 real images left\n",
            "17616 real images left\n",
            "17488 real images left\n",
            "17360 real images left\n",
            "17232 real images left\n",
            "17104 real images left\n",
            "16976 real images left\n",
            "16848 real images left\n",
            "16720 real images left\n",
            "16592 real images left\n",
            "16464 real images left\n",
            "16336 real images left\n",
            "16208 real images left\n",
            "16080 real images left\n",
            "15952 real images left\n",
            "15824 real images left\n",
            "15696 real images left\n",
            "15568 real images left\n",
            "15440 real images left\n",
            "15312 real images left\n",
            "15184 real images left\n",
            "15056 real images left\n",
            "14928 real images left\n",
            "14800 real images left\n",
            "14672 real images left\n",
            "14544 real images left\n",
            "14416 real images left\n",
            "14288 real images left\n",
            "14160 real images left\n",
            "14032 real images left\n",
            "13904 real images left\n",
            "13776 real images left\n",
            "13648 real images left\n",
            "13520 real images left\n",
            "13392 real images left\n",
            "13264 real images left\n",
            "13136 real images left\n",
            "13008 real images left\n",
            "12880 real images left\n",
            "12752 real images left\n",
            "12624 real images left\n",
            "12496 real images left\n",
            "12368 real images left\n",
            "12240 real images left\n",
            "12112 real images left\n",
            "11984 real images left\n",
            "11856 real images left\n",
            "11728 real images left\n",
            "11600 real images left\n",
            "11472 real images left\n",
            "11344 real images left\n",
            "11216 real images left\n",
            "11088 real images left\n",
            "10960 real images left\n",
            "10832 real images left\n",
            "10704 real images left\n",
            "10576 real images left\n",
            "10448 real images left\n",
            "10320 real images left\n",
            "10192 real images left\n",
            "10064 real images left\n",
            "9936 real images left\n",
            "9808 real images left\n",
            "9680 real images left\n",
            "9552 real images left\n",
            "9424 real images left\n",
            "9296 real images left\n",
            "9168 real images left\n",
            "9040 real images left\n",
            "8912 real images left\n",
            "8784 real images left\n",
            "8656 real images left\n",
            "8528 real images left\n",
            "8400 real images left\n",
            "8272 real images left\n",
            "8144 real images left\n",
            "8016 real images left\n",
            "7888 real images left\n",
            "7760 real images left\n",
            "7632 real images left\n",
            "7504 real images left\n",
            "7376 real images left\n",
            "7248 real images left\n",
            "7120 real images left\n",
            "6992 real images left\n",
            "6864 real images left\n",
            "6736 real images left\n",
            "6608 real images left\n",
            "6480 real images left\n",
            "6352 real images left\n",
            "6224 real images left\n",
            "6096 real images left\n",
            "5968 real images left\n",
            "5840 real images left\n",
            "5712 real images left\n",
            "5584 real images left\n",
            "5456 real images left\n",
            "5328 real images left\n",
            "5200 real images left\n",
            "5072 real images left\n",
            "4944 real images left\n",
            "4816 real images left\n",
            "4688 real images left\n",
            "4560 real images left\n",
            "4432 real images left\n",
            "4304 real images left\n",
            "4176 real images left\n",
            "4048 real images left\n",
            "3920 real images left\n",
            "3792 real images left\n",
            "3664 real images left\n",
            "3536 real images left\n",
            "3408 real images left\n",
            "3280 real images left\n",
            "3152 real images left\n",
            "3024 real images left\n",
            "2896 real images left\n",
            "2768 real images left\n",
            "2640 real images left\n",
            "2512 real images left\n",
            "2384 real images left\n",
            "2256 real images left\n",
            "2128 real images left\n",
            "2000 real images left\n",
            "1872 real images left\n",
            "1744 real images left\n",
            "1616 real images left\n",
            "1488 real images left\n",
            "1360 real images left\n",
            "1232 real images left\n",
            "1104 real images left\n",
            "976 real images left\n",
            "848 real images left\n",
            "720 real images left\n",
            "592 real images left\n",
            "464 real images left\n",
            "336 real images left\n",
            "208 real images left\n",
            "80 real images left\n",
            "0 real images left\n",
            "FID: 86.1705093383789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optional: save images\n",
        "\n",
        "output_filename = 'cifar10-2.jpg' # @param {type:\"string\"}\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "\n",
        "def save_images(images, path, **kwargs):\n",
        "    grid = make_grid(images, **kwargs)\n",
        "    arr = grid.permute(1, 2, 0).cpu().numpy()\n",
        "    im = Image.fromarray(arr)\n",
        "    im.save(path)\n",
        "\n",
        "save_images(generated_samples, output_filename)\n",
        "print('Successfully saved images.')"
      ],
      "metadata": {
        "id": "P4kd3bFbQita",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6cf759d5b4cc423a96b1d944fe05883b",
            "60f47695ddaa4ce5ab17d297541c0107",
            "3d141a0355d84966bdd350f3f4b29aa8",
            "bb9b4cade5374fa79bcdcbe319ada058",
            "39dcdcd64ba4422eb3faff113d9c1b5f",
            "f4139f05931542919403440e427216a1",
            "34b627b705bb4e8b9ba57e25d4b8e8c3",
            "82ca580571bb4eb2aa1779179946aac6",
            "3eb45f21b62f48c89b7652d273099924",
            "3879bea10a234cf6a70d496fcb6549f6",
            "a78709ee4e4343d7aed04b00a1bc0258"
          ]
        },
        "outputId": "ac85318c-0c89-4404-8e2b-38b96e73d818",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cf759d5b4cc423a96b1d944fe05883b"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}